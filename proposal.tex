\documentclass[12pt]{article}
\usepackage{pdflscape}
\usepackage{pgfgantt}
\usepackage{afterpage}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{hyperref}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE University of Nebraska --- Lincoln}\\[1.5cm] % Name of your university/college
\textsc{\Large Graduate Independent Study Proposal}\\[0.5cm] % Major heading such as course name
\textsc{\large Summer 2017}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Machine Learning}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student:}\\
Kelly \textsc{Boswell} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Dr. Yi \textsc{Xian} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

\end{titlepage}

\newpage
re
\pagenumbering{arabic}

\section{Justification}

The subject of Machine Learning is concerned with the design, analysis, implementation
and applications of computer programs that learn from experience. This independent study is designed
to be a comprehensive survey of Machine Learning with many hands-on applications. It begins
with a brief overview of the field of Machine Learning and then a review of Probability
Theory and Statistics before diving into the Machine Learning algorithms of increasing complexity.
This course will be invaluable for my future studies in Electrical and Computer Engineering as
I intend to apply one or more Machine Learning algorithms throughout my Graduate career, including for my
Master's Thesis and beyond. 

\subsection{Course Prerequisites}

Some prerequisite knowledge for Machine Learning are:

\begin{itemize}
\item Calculus
\item Probability and Statistics
\item Linear Algebra
\item Advanced Computer Programming
\end{itemize}

All of the math prerequisites are covered by my Bachelors degree in Electronics Engineering from UNL. The homework
problems and lab assignments will also require some relatively advanced programming skills. I am a Senior Software
Engineer that has designed and developed several enterprise applications so I have attained all prerequisite
programming skills in my professional career.

\subsection{Similar ECE Courses}

The most closely related ECE course offering is CEEN-8800 (Random Processes in Engineering) but there is very little
overlap between these topics and both may be learned independently with a basic Probability and Statistics course
(e.g. STAT-3800) as a prerequisite for each.

\subsection{Similar Non-ECE Courses}

There is a Computer Science course (CSCI 8456) that introduces Machine Learning. However, it's a
broad overview of the entire field of Artificial Intelligence of which Machine Learning is only a subset.
As such, Machine Learning is only given a shallow introduction in this course. 

There is another Computer Science course (CSCI 8476) that does appear to cover Machine Learning in a great deal of depth.
However, it doesn't appear to be offered often. The last course offering appears to have been in Fall 2014. However, I will email
the course instructor and see if it's possible for me to take this course. Its only two required courses are CSCI 1620 and MATH
2050, which I have taken.

Machine Learning is also very closely related to Probability and Statistics, so MATH 4750 (Probability and Statistics II) would
cover most of the same mathematics. However, both fields have evolved concurrently and independently of each other, although the
two seem to be merging more and more over time. Even still, there are still enough differences between the two fields that they
remain as separate disciplines.

\section{Objectives}

\begin{enumerate}
\item Be able to formulate machine learning problems corresponding to different applications. 
\item Understand a range of machine learning algorithms along with their strengths and weaknesses. 
\item Understand the basic theory underlying machine learning. 
\item Be able to aply machine learning algorithms to solve problems of moderate complexity. 
\item Be able to read current research papers and understand the issues raised by current research.
\end{enumerate}

\section{Process}

This course will cover some significant literature review, drawn mostly from the text book \textit{Machine Learning: A
Probabilistic Perspective} by Kevin Murphy.  Some selected exercises will be worked from this text book each week which may
include some programming problems. Furthermore, some selected laboratory exercises will be chosen from the text book
\textit{Advanced Analytics with Spark: Patterns for Learning from Data at Scale}. There will also be some required reading from
recently published research papers.

\section{Implications}

I intend to use one or more of the techniques from this course to develop some advanced intrusion detection methodologies for my
Master's Thesis.

\section{Suggested Support Material}

The primary text books for this course are \textit{Machine Learning: A Probabilistic Perspective} by Kevin Murphy and
\textit{Advanced Analytics with Spark: Patterns for Learning from Data at Scale} by Sandy Ryza, Uri Laserson, and Sean Owen.
Mr. Murphy is a highly cited Machine Learning researcher and his text book appears to be both broad and deep in its
coverage of the theory behind Machine Learning. This book's \href{https://mitpress.mit.edu/books/machine-learning-0}{companion website}
also offers Instructor Resources which course instructors may request via an online form. The book by Ryza, Laserson, and Owen
is for laboratory explorations of the topic. Apache Spark is a popular engine for large-scale data processing that is packaged
with its very own Machine Learning library, MLib. Apache Spark supports the Java, Scala, Python, and R programming languages.

I have also listed some supplementary text books in the event that I need to read about a topic from a different point
of view. These text books will also serve as great desktop references for future studies and research. Furthermore, most of them
are freely available online, except the primary text (Murphy) and the Bishop book.

\subsection{Software Platforms and Libraries}

There are also several different competing and freely availabe Machine Learning libraries in a range of different languages that
may be more suitable for a given homework problem or a given laboratory assignment, since each of these libraries offer their
own strengths and weaknesses. Given that's the case and that it won't be known which library is more suitable for the application
up front, homework problems that require some programming to be completed or lab assignments will not specify which language or
library to use. However, in general, the preferred platform for homework assignments will be MATLAB and the text book's MATLAB
machine learning library that is freely distributed online, while the preferred platform for lab assignments will be Apache
Spark.

\subsection{Text Book Listings}

Text books for theoretical coverage:
\begin{itemize}
\item \textbf{Primary} --- Kevin Murphy, \textit{Machine Learning: A Probabilistic Perspective}, MIT Press
\item Supplementary --- Christopher M. Bishop, \textit{Pattern Recognition and Machine Learning}, Springer
\item Supplementary --- David J. C. MacKay, \textit{Information Theory, Inference, and Learning Algorithms}, Cambridge University Press
\item Supplementary --- Trevor Hastie, Robert Tibshirani, and Jerome Friedman, \textit{The Elements of Statistical Learning}, Springer
\item Supplementary --- David Barber, \textit{Bayesian Reasoning and Machine Learning}, Cambridge University Press
\end{itemize}

Text books for laboratory explorations:
\begin{itemize}
\item \textbf{Primary} --- Sandy Ryza, Uri Laserson, and Sean Owen, \textit{Advanced Analytics with Spark: Patterns for Learning from Data at Scale}, O'Reilly Media 
\end{itemize}

\section{Time Line}

Figures \ref{fig:gchart1}, \ref{fig:gchart2}, \ref{fig:gchart3}, and \ref{fig:gchart4} depict the
proposed learning schedule for this course. The schedule for individual reading assignments are listed in Tables
\ref{table:readings1}, \ref{table:readings2}, \ref{table:readings3}, and \ref{table:readings4}.

\section{Assessment Plan}

Throughout the course, solutions for some selected exercises from the primary text book, \textit{Machine Learning: A Probabilistic
Perspective}, will be submitted to the course supervisor. This book's 
\href{https://mitpress.mit.edu/books/machine-learning-0}{companion website} has an online request form for Instructor Materials
that the course supervisor may use to attain all exercise solutions. The lab exercises are selected from the
other primary text book, \textit{Advanced Analytics with Spark: Patterns for Learning from Data at Scale}. This book points to
data sets and offers a basic approach to each programming problem but it does not offer a complete solution to any of the
problems.  It also offers results at some
intermediate and final stages so the course instructor may make some objective comparisons between these results in the book and
the results offered in the lab reports. The lab reports will include all code used and step-by-step instructions to reproduce the
results.

\afterpage{
    % Gant Chart for weeks 1-3
    \begin{figure}[h]
    \caption{Gantt Chart of Weeks 1-3 of Study}
    \label{fig:gchart1}
    \centering
    \begin{ganttchart}[
            hgrid,
            vgrid,
            x unit=5mm,
            time slot format=isodate
    ]{2017-05-15}{2017-06-04}
        \gantttitlecalendar{year, month, day, week=1, weekday} \\
        \ganttgroup{Introduction}{2017-05-15}{2017-05-18} \\
        \ganttbar{Reading 1}{2017-05-15}{2017-05-16} \\
        \ganttlinkedbar{Reading 2}{2017-05-17}{2017-05-18} \\
        \ganttgroup{Linear Models}{2017-05-18}{2017-06-04} \\
        \ganttbar{Reading 3}{2017-05-22}{2017-05-23} \\
        \ganttlinkedbar{Reading 4}{2017-05-26}{2017-05-27} \\ 
        \ganttlinkedbar{Reading 5}{2017-05-29}{2017-05-30} \\ 
        \ganttlinkedbar{Reading 6}{2017-06-02}{2017-06-03}
    \end{ganttchart}
    \end{figure}

    % Gant Chart for weeks 4-6
    \begin{figure}[h]
    \caption{Gantt Chart of Weeks 4-6 of Study}
    \label{fig:gchart2}
    \centering
    \begin{ganttchart}[
            hgrid,
            vgrid,
            x unit=5mm,
            time slot format=isodate
    ]{2017-06-05}{2017-06-25}
        \gantttitlecalendar{year, month, day, week=4, weekday} \\
        \ganttgroup{Linear Models}{2017-06-05}{2017-06-06} \\
        \ganttbar{Reading 7}{2017-06-05}{2017-06-06} \\
        \ganttgroup{Unsupervised Bayesian Modeling}{2017-06-07}{2017-06-18} \\
        \ganttbar{Reading 8}{2017-06-07}{2017-06-08} \\
        \ganttlinkedbar{Reading 9}{2017-06-09}{2017-06-10} \\
        \ganttlinkedbar{Reading 10}{2017-06-12}{2017-06-13} \\
        \ganttlinkedbar{Reading 11}{2017-06-14}{2017-06-15} \\
        \ganttlinkedbar{Reading 12}{2017-06-16}{2017-06-17} \\
        \ganttgroup{Inference Procedures}{2017-06-19}{2017-06-25} \\
        \ganttbar{Reading 13}{2017-06-19}{2017-06-20} \\
        \ganttlinkedbar{Reading 14}{2017-06-21}{2017-06-22} \\
        \ganttlinkedbar{Reading 15}{2017-06-23}{2017-06-24}
    \end{ganttchart}
    \end{figure}

    % Gant Chart for weeks 7-9
    \begin{figure}[h]
    \caption{Gantt Chart of Weeks 7-9 of Study}
    \label{fig:gchart3}
    \centering
    \begin{ganttchart}[
            hgrid,
            vgrid,
            x unit=5mm,
            time slot format=isodate
    ]{2017-06-26}{2017-07-16}
        \gantttitlecalendar{year, month, day, week=7, weekday} \\
        \ganttgroup{Inference Procedures}{2017-06-26}{2017-07-08} \\
        \ganttbar{Reading 16}{2017-06-26}{2017-06-27} \\
        \ganttlinkedbar{Reading 17}{2017-06-28}{2017-06-29} \\
        \ganttlinkedbar{Reading 18}{2017-07-03}{2017-07-04} \\
        \ganttlinkedbar{Reading 19}{2017-07-05}{2017-07-06} \\
        \ganttlinkedbar{Reading 20}{2017-07-07}{2017-07-08} \\
        \ganttgroup{Example Models}{2017-07-10}{2017-07-16} \\
        \ganttbar{Reading 21}{2017-07-10}{2017-07-11} \\
        \ganttlinkedbar{Reading 22}{2017-07-12}{2017-07-13} \\
        \ganttlinkedbar{Reading 23}{2017-07-14}{2017-07-15} \\
    \end{ganttchart}
    \end{figure}

    % Gant Chart for weeks 10-13
    \begin{figure}[h]
    \caption{Gantt Chart of Weeks 10-13 of Study}
    \label{fig:gchart4}
    \centering
    \begin{ganttchart}[
            hgrid,
            vgrid,
            x unit=5mm,
            time slot format=isodate
    ]{2017-07-17}{2017-08-11}
        \gantttitlecalendar{year, month, day, week=10, weekday} \\
        \ganttgroup{Nonparametric Models}{2017-07-17}{2017-07-30} \\
        \ganttbar{Reading 24}{2017-07-17}{2017-07-18} \\
        \ganttlinkedbar{Reading 25}{2017-07-19}{2017-07-20} \\
        \ganttlinkedbar{Reading 26}{2017-07-21}{2017-07-22} \\
        \ganttlinkedbar{Reading 27}{2017-07-24}{2017-07-25} \\
        \ganttlinkedbar{Reading 28}{2017-07-26}{2017-07-27} \\
        \ganttlinkedbar{Reading 29}{2017-07-28}{2017-07-29} \\
        \ganttgroup{Deep Learning}{2017-07-31}{2017-08-11} \\
        \ganttbar{Reading 30}{2017-07-31}{2017-08-01} \\
        \ganttlinkedbar{Reading 31}{2017-08-02}{2017-08-03} \\
        \ganttlinkedbar{Reading 32}{2017-08-04}{2017-08-05}
    \end{ganttchart}
    \end{figure}

    \begin{table}
    \caption{Readings}
    \label{table:readings1}
    \begin{center}
        \begin{tabular}{| c | l | l | c | p{6 cm} |}
            \hline
            Reading & Source & Chapter & Sections & Chapter Title \\
            \hline
            \multicolumn{5}{|c|}{Introduction} \\
            \hline
            \multirow{2}{*}{1} & Book: Murphy & Chapter 1 & All & Introduction \\
                               & \textit{Book: Bishop} & \textit{Chapter 1} & \textit{All} & \textit{Introduction} \\
            \hline
            \multirow{3}{*}{2} & Book: Murphy & Chapter 2 & All & Probability \\
                               & Book: Murphy & Chapter 3 & All & Generative Models for Discrete Data \\
                               & \textit{Book: Bishop} & \textit{Chapter 2} & \textit{2.1-2.2} & \textit{Probability Distributions} \\
            \hline
            \multicolumn{5}{|c|}{Linear Models} \\
            \hline
            \multirow{2}{*}{3} & Book: Murphy & Chapter 4 & All & Gaussian Models \\
                               & \textit{Book: Bishop} & \textit{Chapter 2} & \textit{2.3} & \textit{Probability Distributions} \\
            \hline
            \multirow{2}{*}{4} & Book: Murphy & Chapter 5 & All & Bayesian Statistics \\
                               & \textit{Book: Murphy} & \textit{Chapter 6} & \textit{All} & \textit{Frequentist Statistics} \\
            \hline
            \multirow{2}{*}{5} & Book: Murphy & Chapter 7 & All & Linear Regression \\
                               & \textit{Book: Bishop} & \textit{Chapter 3} & \textit{All} & \textit{Linear Models for Regression} \\
            \hline
            \multirow{2}{*}{6} & Book: Murphy & Chapter 8 & All & Logistic Regression \\
                               & \textit{Book: Bishop} & \textit{Chapter 4} & \textit{All} & \textit{Linear Models for Classification} \\
            \hline
            \multirow{2}{*}{7} & Book: Murphy & Chapter 9 & 1-4 & Generalized Linear Models \\
                               & \textit{Book: Murphy} & \textit{Chapter 9} & \textit{5-7} & \textit{Generalized Linear Models} \\
            \hline
            \multicolumn{5}{|c|}{Unsupervised Bayesian Modeling} \\
            \hline
            \multirow{2}{*}{8} & Book: Murphy & Chapter 19 & 1-4 & Undirected Graphical Models \\
                               & \textit{Book: Bishop} & \textit{Chapter 8} & \textit{1-3} & \textit{Graphical Models} \\
            \hline
            \multirow{2}{*}{9} & Book: Murphy & Chapter 10 & 1-5 & Directed Graphical Models \\
                               & \textit{Book: Bishop} & \textit{Chapter 8} & \textit{4} & \textit{Graphical Models} \\
            \hline
            \multirow{2}{*}{10} & Book: Murphy & Chapter 11 & All & Mixture Models and the EM Algorithm \\
                               & \textit{Book: Bishop} & \textit{Chapter 9} & \textit{All} & \textit{Mixture Models and EM} \\
            \hline
            \multirow{3}{*}{11} & Book: Murphy & Chapter 12 & All & Latent Linear Models \\
                                & Paper: Roweis & & & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/lds.pdf}{A Unifying Review of Linear Gaussian Models} \\
                               & \textit{Book: Bishop} & \textit{Chapter 12} & \textit{1-2} & \textit{Continuous Latent Variables} \\
            \hline
            \multirow{2}{*}{12} & Book: Murphy & Chapter 13 & 1-7 & Sparse Linear Models \\
                               & \textit{Book: Hastie} & \textit{Chapter 3} & \textit{All} & \textit{Linear Methods for Regression} \\
            \hline
        \end{tabular}
    \end{center}
    \end{table}

    \begin{table}
    \caption{Readings}
    \label{table:readings2}
    \begin{center}
        \begin{tabular}{| c | l | l | c | p{6 cm} |}
            \hline
            Reading & Source & Chapter & Sections & Chapter Title \\
            \hline
            \multicolumn{5}{|c|}{Inference Procedures} \\
            \hline
            \multirow{5}{*}{13} & Book: Murphy & Chapter 17 & 1-4 & Markov and Hidden Markov Models \\
                               & Book: Murphy & Chapter 20 & 1-3 & Exact Inference for Graphical Models \\
                               & \textit{Book: Murphy} & \textit{Chapter 17} & \textit{5-6} & \textit{Markov and Hidden Markov Models} \\
                               & \textit{Book: Bishop} & \textit{Chapter 8} & \textit{4} & \textit{Graphical Models} \\
                               & \textit{Book: Bishop} & \textit{Chapter 13} & \textit{1-2} & \textit{Sequential Data} \\
            \hline
            \multirow{1}{*}{14} & Book: Murphy & Chapter 20 & 4 & Exact Inference for Models \\
            \hline
            \multirow{2}{*}{15} & Book: Murphy & Chapter 21 & 1-3,5-6 & Variational Inference \\
                               & \textit{Book: Bishop} & \textit{Chapter 10} & \textit{All} & \textit{Approximate Inference} \\
            \hline
            \multirow{1}{*}{16} & Book: Murphy & Chapter 22 & 1-2,5-6 & More Variational Inference \\
            \hline
            \multirow{2}{*}{17} & Book: Murphy & Chapter 23 & 1-4 & Monte Carlo Inference \\
                               & \textit{Book: Bishop} & \textit{Chapter 11} & \textit{1} & \textit{Sampling Methods} \\
            \hline
            \multirow{2}{*}{18} & Book: Murphy & Chapter 23 & 5-6 & Monte Carlo Inference \\
                               & \textit{Book: Bishop} & \textit{Chapter 13} & \textit{3} & \textit{Sequential Data} \\
            \hline
            \multirow{2}{*}{19} & Book: Murphy & Chapter 24 & 1-4 & Markov Chain Monte Carlo (MCMC) Inference \\
                               & \textit{Book: Bishop} & \textit{Chapter 11} & \textit{2-3} & \textit{Sampling Methods} \\
            \hline
            \multirow{4}{*}{20} & Book: Murphy & Chapter 24 & 5-7 & Markov Chain Monte Carlo (MCMC) Inference \\
                               & Paper: Neal & & & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/neal-2001.pdf}{Slice Sampling} \\
                               & Paper: Neal & & & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/neal-2010.pdf}{MCMC using Hamiltonian dynamics} \\
                               & \textit{Book: Bishop} & \textit{Chapter 11} & \textit{4-6} & \textit{Sampling Methods} \\
            \hline
        \end{tabular}
    \end{center}
    \end{table}

    \begin{table}
    \caption{Readings}
    \label{table:readings3}
    \begin{center}
        \begin{tabular}{| c | l | l | c | p{6 cm} |}
            \hline
            Reading & Source & Chapter & Sections & Chapter Title \\
            \hline
            \multicolumn{5}{|c|}{Example Models} \\
            \hline
            \multirow{4}{*}{21} & Book: Murphy & Chapter 27 & 1-4 & Latent Variable Models for Discrete Data \\
                                & Paper: Blei &  &  & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/blei-ng-jordan-2003.pdf}{Latent Dirichlet Allocation} \\
                                & Paper: Blei &  &  & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/blei-lafferty-2009.pdf}{Topic Models} \\
                                & Paper: Griffiths &  &  & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/griffiths-steyvers-2004.pdf}{Finding Scientific Topics} \\
            \hline
            \multirow{3}{*}{22} & Book: Murphy & Chapter 18 & 1-4 & State Space Models \\
                               & \textit{Book: Murphy} & \textit{Chapter 18} & \textit{5-6} & \textit{State Space Models} \\
                               & \textit{Book: Bishop} & \textit{Chapter 13} & \textit{3} & \textit{Sequential Data} \\
            \hline
            \multirow{1}{*}{23} & Book: Murphy & Chapter 27 & 5-6 & Latent Variable Models for Discrete Data \\
            \hline
            \multicolumn{5}{|c|}{Nonparametric Models} \\
            \hline
            \multirow{2}{*}{24} & Book: Murphy & Chapter 14 & All & Kernels \\
                               & \textit{Book: Bishop} & \textit{Chapter 6} & \textit{1-2} & \textit{Kernel Methods} \\
            \hline
            \multirow{3}{*}{25} & Book: Murphy & Chapter 15 & All & Gaussian Processes \\
                               & Video: MacKay &  &  & \href{http://videolectures.net/gpip06_mackay_gpb/}{Gaussian Process Basics} \\
                               & \textit{Book: Bishop} & \textit{Chapter 6} & \textit{4} & \textit{Kernel Methods} \\
            \hline
            \multirow{2}{*}{26} & Paper: Murray &  &  & \href{http://homepages.inf.ed.ac.uk/imurray2/pub/10ess/ess.pdf}{Elliptical Slice Sampling} \\
                                & Paper: Murray &  &  & \href{http://homepages.inf.ed.ac.uk/imurray2/pub/10hypers/hypers.pdf}{Slice Sampling Covariance Hyperparameters of Latent Gaussian Models} \\
            \hline
            \multirow{3}{*}{27} & Book: Murphy & Chapter 25 & 1-2 & Clustering \\
                                & Paper: Orbanz &  &  & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/orbanz-teh-2010.pdf}{Bayesian Nonparametric Models} \\
                                & Paper: Rasmussen &  &  & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/rasmussen-1999a.pdf}{The Infinite Gaussian Mixture Model} \\
            \hline
            \multirow{1}{*}{28} & Paper: Sethuraman &  &  & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/sethuraman-1994.pdf}{A Constructive Definition of Dirichlet Priors} \\
            \hline
            \multirow{1}{*}{29} & Paper: Neal &  &  & \href{http://web-static-aws.seas.harvard.edu/courses/cs281/papers/neal-1998.pdf}{Markov Chain Sampling Methods for Dirichlet Process Mixture Models} \\
            \hline
        \end{tabular}
    \end{center}
    \end{table}

    \begin{table}
    \caption{Readings}
    \label{table:readings4}
    \begin{center}
        \begin{tabular}{| c | l | l | c | p{6 cm} |}
            \hline
            Reading & Source & Chapter & Sections & Chapter Title \\
            \hline
            \multicolumn{5}{|c|}{Deep Learning} \\
            \hline
            \multirow{2}{*}{30} & Book: Murphy & Chapter 27 & 7 & Latent Variable Models for Discrete Data \\
                                & Book: Murphy & Chapter 28 & 1 & Deep Learning \\
            \hline
            \multirow{2}{*}{31} & Book: Murphy & Chapter 16 & 5 & Adaptive Basis Function Models \\
                                & Book: Bishop & Chapter 5 & All & Neural Networks \\
            \hline
            \multirow{1}{*}{32} & Book: Murphy & Chapter 28 & 3-5 & Deep Learning \\
            \hline
        \end{tabular}
    \end{center}
    \end{table}

    \clearpage
}

\end{document}
